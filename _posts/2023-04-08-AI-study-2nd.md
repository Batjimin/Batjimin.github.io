---
layout: post
title: AI Study 2nd
subtitle: GDG송도 AI예술작품스터디 2주차
categories: study
tags: [AI]
---

<h3>GDG송도 AI예술작품스터디 2주차</h3>

  *수정예정*<br>
<div>
AlexNet(2012) : GPU를 처음으로 사용 <br>
AutoEncoder (1987) : DNN으로만 이루어져 있음<br>
U-Net(2015)<br>
DCGAN(2015) : Prototype of GAN<br></div>

<div>
생성모델 1번째 원리 : 이미지 압축 후 푸는 행위<br>
생성모델 2번째 원리 : 모델이 어떻게 변하는지 확인하기 <br>
DeepDream(2015) : 딥러닝 내부에서 만들어진 이미지들을 추출해 확대 <br>
Diffusion model(2020) : 가우스분포 따르는 노이즈 지속적으로 추가,<br> 이후 노이즈 하나씩 제거하면서 이미지 복원 <br>
  압축 푸는 대신 노이즈 추가. (학습된 w값에 의해 원본 그림을 복원) <br>
 참고자료 : 모두의 연구소 정지용 교수님, 모두팝 diffusion <br></div>
 
<div>
RNN을 좋게 만드려는 시도: LSTM(장단기 기억저장소) -> GRU(2014)<br>
Seq2Sseq(2014) : Sequence to Sequence. *eos - end of sequence 
Seq2seq의 문제점: 앞문장의 정보 소멸 이슈, 중요 데이터 구별의 어려움
Seq2seeq with Attention(2014) : Seq2seq모델의 문제점 해결 위함. 한 context벡터 생성 시 해당 시점(중요부분)에 점수를 더 줄 수 있도록 함.<br> Attention을 잘 나오게 하는 방법 고민하게 된 계기. <br> Ex) Bahdanau Attention, Luong Attention<br>
  <span>
    Transformer(2017) : 현대 딥러닝의 AtoZ. 자연어처리 평정. <br>
     Ex) 이미지 : Vision Transformer<br>
     Encoder의 기능 : 문장 맥락 이해
     Decoder의 기능 : 이해한 맥락 바탕으로 글 생성
     *이 Encoder, Decoder기능은 GPT모델에도 적용
  </span>
  <span>
    BERT(2018) : pre-training => fine-Tuning방식. 
    GPT(2018) : In-contezt learning 프롬프트 입력-> 규칙 작성 -> 주어진 규칙의 맥락 읽어 작동.
     *zeroshot : 별도 예시(프롬프트)제공 없이 작동 (oneshot : 한 가지 예시만으로 작동가능)
    GPT 3.5(2022) : Human scoring 과정 추가. 평가 결과가 reward모델에 들어감. 일종의 필터링 기능을 함.
  </span>
  자연어처리 모델<br>
  1. 이미지 생성모델의 Encoder-Decoder차용
</div>
 
 

{% comment %}
Might you have an include in your theme? Why not try it here!
{% include my-themes-great-include.html %}
{% endcomment %}
