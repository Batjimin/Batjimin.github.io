---
layout: post
categories: study
tags: [study], [AI]
---

<h3>GDG송도 AI예술작품스터디 1주차</h3>

  *수정예정*<br>
퍼셉트론 :
	Step Function(y) = 1 (x>=a)
			  0 (x<a)
	이전노드 output = 현재노드 input 
	weight 통과하면서 값 커지거나 작아짐
	input에 각 weight를 곱한 값을 모두 더해 계단함수(활성화함수)를 통과해 output값 생성
	weight값 조정으로 원하는 결과 생성

	EX) 거리 x당 요금 y (딥러닝- 선형회귀에도 적용)
	input(입력값) : x0,x1,x2 – 입력노드에서는 학습데이터, 그 외는 이전 노드에서의 output 
	weight가중치 : w0,w1,w2 – 학습되는(최적화되는) 값 , 1차원에서는 직선기울기
	WeightedSum(가중치입력 합) 
		: w0x0+ w1x1+ w2x2 – 활성화함수의 입력값. 1차원에서는 W*X
	Activation Function(활성화함수) : WeightedSum을 출력신호로 변환.
		회귀의 출력노드에선 항등함수사용(입력이 출력되는 함수)
			why? 수치 값 예측 위해
		요즘은 ReLU 계열 사용 – Vanishing gradient문제 최소화
		*Vanishing Gradient 은닉층이 많은, 깊은 망 학습시킬 때 
		OUTPUT에서 멀어질수록 학습 안되는 현상
		활성화 함수로 sigmoid 사용하여 발생.
		*sigmoid는 0~1 출력. 미분하면 0~0.25. 1/4씩 감소하면 0에 수렴->학습불가.
	**W,b값을 변경하여 그래프의 결과 조금씩 변경. 
	데이터와 그래프 사이 오차가 가장 작은 그래프 W, b 찾기.**
	XOR exclusive-or 구현은 두 개의 퍼셉트론 필요. 사각형의(왼쪽 하단 모서리 좌표 0,0) 대가선을 중심으로 양 쪽에 두 개의 분할선 그으면 구현가능. 

{% comment %}
Might you have an include in your theme? Why not try it here!
{% include my-themes-great-include.html %}
{% endcomment %}
